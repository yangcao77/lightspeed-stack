services:
  # Mock MCP server for testing
  mcp-mock-server:
    build:
      context: .
      dockerfile: dev-tools/mcp-mock-server/Dockerfile
    container_name: mcp-mock-server
    ports:
      - "3000:3000"
    networks:
      - lightspeednet
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 5s

  # Red Hat llama-stack distribution with FAISS
  llama-stack:
    build:
      context: .
      dockerfile: test.containerfile
    platform: linux/amd64
    container_name: llama-stack
    ports:
      - "8321:8321"  # Expose llama-stack on 8321 (adjust if needed)
    volumes:
      - ./run.yaml:/opt/app-root/run.yaml:z
      - ${GCP_KEYS_PATH:-./tmp/.gcp-keys-dummy}:/opt/app-root/.gcp-keys:ro
      - ./lightspeed-stack.yaml:/opt/app-root/lightspeed-stack.yaml:ro
      - llama-storage:/opt/app-root/src/.llama/storage
      - ./tests/e2e/rag:/opt/app-root/src/.llama/storage/rag:z
    environment:
      - BRAVE_SEARCH_API_KEY=${BRAVE_SEARCH_API_KEY:-}
      - TAVILY_SEARCH_API_KEY=${TAVILY_SEARCH_API_KEY:-}
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - E2E_OPENAI_MODEL=${E2E_OPENAI_MODEL:-gpt-4o-mini}
      # Azure Entra ID credentials (AZURE_API_KEY is passed via provider_data at request time)
      - TENANT_ID=${TENANT_ID:-}
      - CLIENT_ID=${CLIENT_ID:-}
      - CLIENT_SECRET=${CLIENT_SECRET:-}
      # RHAIIS
      - RHAIIS_URL=${RHAIIS_URL}
      - RHAIIS_API_KEY=${RHAIIS_API_KEY}
      - RHAIIS_MODEL=${RHAIIS_MODEL}
      # RHEL AI
      - RHEL_AI_URL=${RHEL_AI_URL}
      - RHEL_AI_PORT=${RHEL_AI_PORT}
      - RHEL_AI_API_KEY=${RHEL_AI_API_KEY}
      - RHEL_AI_MODEL=${RHEL_AI_MODEL}
      # VertexAI
      - GOOGLE_APPLICATION_CREDENTIALS=${GOOGLE_APPLICATION_CREDENTIALS:-}
      - VERTEX_AI_PROJECT=${VERTEX_AI_PROJECT:-}
      - VERTEX_AI_LOCATION=${VERTEX_AI_LOCATION:-}
      # WatsonX
      - WATSONX_BASE_URL=${WATSONX_BASE_URL:-}
      - WATSONX_PROJECT_ID=${WATSONX_PROJECT_ID:-}
      - WATSONX_API_KEY=${WATSONX_API_KEY:-}
      # Enable debug logging if needed
      - LLAMA_STACK_LOGGING=${LLAMA_STACK_LOGGING:-}
    networks:
      - lightspeednet
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8321/v1/health"]
      interval: 10s   # how often to run the check
      timeout: 5s    # how long to wait before considering it failed
      retries: 3      # how many times to retry before marking as unhealthy
      start_period: 15s # time to wait before starting checks

  lightspeed-stack:
    build:
      context: .
      dockerfile: Containerfile
    container_name: lightspeed-stack
    ports:
      - "8080:8080"
    volumes:
      - ./lightspeed-stack.yaml:/app-root/lightspeed-stack.yaml:z
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Azure Entra ID credentials (AZURE_API_KEY is obtained dynamically)
      - TENANT_ID=${TENANT_ID:-}
      - CLIENT_ID=${CLIENT_ID:-}
      - CLIENT_SECRET=${CLIENT_SECRET:-}
    depends_on:
        llama-stack:
          condition: service_healthy
        mcp-mock-server:
          condition: service_healthy
    networks:
      - lightspeednet
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/liveness"]
      interval: 10s   # how often to run the check
      timeout: 5s    # how long to wait before considering it failed
      retries: 3      # how many times to retry before marking as unhealthy
      start_period: 5s # time to wait before starting checks

  # Mock JWKS server for RBAC E2E tests
  mock-jwks:
    build:
      context: ./tests/e2e/mock_jwks_server
      dockerfile: Dockerfile
    container_name: mock-jwks
    ports:
      - "8000:8000"
    networks:
      - lightspeednet
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 2s

volumes:
  llama-storage:

networks:
  lightspeednet:
    driver: bridge
